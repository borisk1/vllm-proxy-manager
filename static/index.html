<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Proxy Manager</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh; padding: 20px;
        }
        .container { max-width: 1400px; margin: 0 auto; }
        .header { display: flex; justify-content: space-between; align-items: center; margin-bottom: 30px; }
        h1 { color: white; font-size: 2.5em; text-shadow: 2px 2px 4px rgba(0,0,0,0.2); }
        .header-actions { display: flex; gap: 10px; }
        .btn-header {
            padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer;
            font-weight: bold; background: white; color: #667eea; transition: all 0.2s;
        }
        .btn-header:hover { transform: scale(1.05); box-shadow: 0 4px 8px rgba(0,0,0,0.2); }
        .models-grid {
            display: grid; grid-template-columns: repeat(auto-fill, minmax(420px, 1fr));
            gap: 20px; margin-bottom: 30px;
        }
        .model-card {
            background: white; border-radius: 10px; padding: 20px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1); transition: transform 0.2s, box-shadow 0.2s;
        }
        .model-card:hover { transform: translateY(-5px); box-shadow: 0 6px 12px rgba(0,0,0,0.15); }
        .model-header {
            display: flex; justify-content: space-between; align-items: center;
            margin-bottom: 15px; padding-bottom: 15px; border-bottom: 2px solid #f0f0f0;
        }
        .model-name { font-size: 1.3em; font-weight: bold; color: #333; }
        .status-badge {
            padding: 5px 15px; border-radius: 20px; font-size: 0.85em;
            font-weight: bold; text-transform: uppercase;
        }
        .status-running { background: #10b981; color: white; }
        .status-paused { background: #f59e0b; color: white; }
        .status-stopped { background: #ef4444; color: white; }
        .model-info { margin-bottom: 15px; font-size: 0.9em; color: #666; }
        .model-info-item { 
            display: flex; justify-content: space-between; padding: 5px 0; 
            border-bottom: 1px solid #f3f4f6;
        }
        .model-info-item .label { font-weight: 600; color: #4b5563; }
        .model-info-item .value { 
            word-break: break-all; text-align: right; max-width: 60%;
            font-family: 'Courier New', monospace; font-size: 0.85em; color: #1f2937;
        }
        .stats-section { background: #f9fafb; border-radius: 8px; padding: 15px; margin: 15px 0; }
        .stats-title { font-weight: bold; color: #4b5563; margin-bottom: 10px; font-size: 1em; }
        .stat-item { display: flex; justify-content: space-between; padding: 5px 0; font-size: 0.9em; }
        .stat-label { color: #6b7280; }
        .stat-value { font-weight: bold; color: #1f2937; }
        .stat-highlight { color: #10b981; font-size: 1.1em; }
        .model-actions { display: flex; gap: 10px; margin-top: 15px; flex-wrap: wrap; }
        .btn {
            flex: 1; min-width: 80px; padding: 10px; border: none; border-radius: 5px;
            cursor: pointer; font-weight: bold; transition: all 0.2s; font-size: 0.9em;
        }
        .btn:hover { transform: scale(1.05); }
        .btn-start { background: #10b981; color: white; }
        .btn-stop { background: #ef4444; color: white; }
        .btn-sleep { background: #f59e0b; color: white; }
        .btn-wake { background: #3b82f6; color: white; }
        .btn-edit { background: #6366f1; color: white; }
        .btn-delete { background: #dc2626; color: white; }
        .add-model-btn {
            position: fixed; bottom: 30px; right: 30px; width: 60px; height: 60px;
            border-radius: 50%; background: #10b981; color: white; font-size: 30px;
            border: none; cursor: pointer; box-shadow: 0 4px 12px rgba(0,0,0,0.2);
            transition: all 0.2s; z-index: 100;
        }
        .add-model-btn:hover { transform: scale(1.1); box-shadow: 0 6px 16px rgba(0,0,0,0.3); }
        .modal {
            display: none; position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            background: rgba(0,0,0,0.5); align-items: center; justify-content: center; z-index: 1000;
        }
        .modal.active { display: flex; }
        .modal-content {
            background: white; border-radius: 10px; padding: 30px;
            width: 90%; max-width: 700px; max-height: 90vh; overflow-y: auto;
        }
        .modal-content h2 { margin-bottom: 20px; color: #333; }
        .form-group { margin-bottom: 20px; }
        .form-group label { display: block; margin-bottom: 5px; font-weight: bold; color: #333; }
        .form-group input, .form-group textarea, .form-group select {
            width: 100%; padding: 10px; border: 2px solid #e5e7eb;
            border-radius: 5px; font-size: 14px;
        }
        .form-group input[type="checkbox"] { width: auto; margin-right: 10px; }
        .checkbox-group { display: flex; align-items: center; }
        .form-actions { display: flex; gap: 10px; margin-top: 20px; }
        .btn-submit {
            background: #10b981; color: white; padding: 12px 30px;
            border: none; border-radius: 5px; cursor: pointer; font-weight: bold;
        }
        .btn-cancel {
            background: #6b7280; color: white; padding: 12px 30px;
            border: none; border-radius: 5px; cursor: pointer; font-weight: bold;
        }
        textarea { font-family: monospace; resize: vertical; }
        .info-badge {
            display: inline-block; padding: 3px 8px; border-radius: 3px;
            font-size: 0.75em; font-weight: bold; margin-left: 5px;
        }
        .badge-exclusive { background: #fecaca; color: #991b1b; }
        .badge-autosleep { background: #dbeafe; color: #1e40af; }
        .badge-visible { background: #d1fae5; color: #065f46; }
        .badge-custom { background: #fef3c7; color: #92400e; }
        .badge-env { background: #e0e7ff; color: #3730a3; }
        .badge-preload { background: #fce7f3; color: #831843; }
        .badge-vllm { background: #ddd6fe; color: #5b21b6; }
        .badge-llamacpp { background: #fecdd3; color: #9f1239; }
        .help-text { font-size: 0.85em; color: #6b7280; margin-top: 5px; }
        .section-title {
            font-size: 1.1em; font-weight: bold; color: #4b5563;
            margin: 25px 0 15px 0; padding-bottom: 8px;
            border-bottom: 2px solid #e5e7eb;
        }
        .backend-specific { display: none; }
        .backend-specific.active { display: block; }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üöÄ LLM Proxy Manager</h1>
            <div class="header-actions">
                <button class="btn-header" onclick="checkHealth()">‚ù§Ô∏è Health</button>
                <button class="btn-header" onclick="showRunning()">üìä Running</button>
                <button class="btn-header" onclick="exportConfig()">üì• Export</button>
                <button class="btn-header" onclick="document.getElementById('importFile').click()">üì§ Import</button>
                <input type="file" id="importFile" style="display: none;" accept=".json" onchange="importConfig(event)">
            </div>
        </div>
        <div class="models-grid" id="modelsGrid"></div>
    </div>

    <button class="add-model-btn" onclick="openAddModal()">+</button>

    <div class="modal" id="modelModal">
        <div class="modal-content">
            <h2 id="modalTitle">Add New Model</h2>
            <form id="modelForm" onsubmit="saveModel(event)">
                <div class="section-title">üì¶ Basic Settings</div>
                <div class="form-group">
                    <label>Model Name *</label>
                    <input type="text" id="modelName" required>
                </div>
                <div class="form-group">
                    <label>Backend *</label>
                    <select id="backend" onchange="toggleBackendSettings()">
                        <option value="vllm">vLLM</option>
                        <option value="llamacpp">llama.cpp</option>
                    </select>
                    <div class="help-text">Choose the inference backend</div>
                </div>
                <div class="form-group">
                    <label>Model Path *</label>
                    <input type="text" id="modelPath" required placeholder="meta-llama/Llama-2-7b-hf or /path/to/model.gguf">
                </div>
                <div class="form-group">
                    <div class="checkbox-group">
                        <input type="checkbox" id="isLocal">
                        <label>Local Model</label>
                    </div>
                </div>
                <div class="form-group">
                    <label>Port *</label>
                    <input type="number" id="port" required placeholder="8001">
                </div>

                <div class="section-title">üê≥ Docker Settings</div>
                <div class="form-group">
                    <label>Docker Image</label>
                    <input type="text" id="dockerImage" placeholder="vllm/vllm-openai:v0.13.0 or ghcr.io/ggerganov/llama.cpp:server">
                    <div class="help-text">Default: vllm/vllm-openai:v0.13.0 (vLLM) or ghcr.io/ggerganov/llama.cpp:server (llama.cpp)</div>
                </div>

                <div class="section-title backend-specific vllm-settings active">‚öôÔ∏è vLLM Settings</div>
                <div class="form-group backend-specific vllm-settings active">
                    <label>GPU Memory (0.0-1.0)</label>
                    <input type="number" id="gpuMemory" step="0.1" min="0" max="1" value="0.9">
                </div>
                <div class="form-group backend-specific vllm-settings active">
                    <label>Max Model Length</label>
                    <input type="number" id="maxModelLen" placeholder="4096">
                </div>
                <div class="form-group backend-specific vllm-settings active">
                    <label>Quantization</label>
                    <input type="text" id="quantization" placeholder="awq, gptq">
                </div>
                <div class="form-group backend-specific vllm-settings active">
                    <label>Tensor Parallel Size</label>
                    <input type="number" id="tensorParallel" value="1" min="1">
                </div>

                <div class="section-title backend-specific llamacpp-settings">‚öôÔ∏è llama.cpp Settings</div>
                <div class="form-group backend-specific llamacpp-settings">
                    <label>Context Size</label>
                    <input type="number" id="contextSize" placeholder="4096" value="4096">
                </div>
                <div class="form-group backend-specific llamacpp-settings">
                    <label>GPU Layers (-ngl)</label>
                    <input type="number" id="gpuLayers" placeholder="35" value="35">
                    <div class="help-text">Number of layers to offload to GPU (-1 for all)</div>
                </div>
                <div class="form-group backend-specific llamacpp-settings">
                    <label>Parallel Requests</label>
                    <input type="number" id="parallelRequests" placeholder="4" value="4">
                </div>

                <div class="section-title">üîß Advanced Options</div>
                <div class="form-group">
                    <div class="checkbox-group">
                        <input type="checkbox" id="preload">
                        <label>üöÄ Preload on Startup</label>
                    </div>
                    <div class="help-text">Auto-start this model when proxy manager starts</div>
                </div>
                <div class="form-group">
                    <div class="checkbox-group">
                        <input type="checkbox" id="exclusive">
                        <label>Exclusive Mode</label>
                    </div>
                </div>
                <div class="form-group">
                    <div class="checkbox-group">
                        <input type="checkbox" id="autoSleep" checked>
                        <label>Auto-Sleep</label>
                    </div>
                </div>
                <div class="form-group">
                    <label>Sleep Timeout (sec)</label>
                    <input type="number" id="sleepTimeout" value="300">
                </div>
                <div class="form-group">
                    <div class="checkbox-group">
                        <input type="checkbox" id="alwaysVisible">
                        <label>Always Visible in OpenWebUI</label>
                    </div>
                </div>

                <div class="section-title">üåç Environment Variables</div>
                <div class="form-group">
                    <label>Custom Environment Variables</label>
                    <textarea id="customEnvVars" rows="5" placeholder="VLLM_SERVER_DEV_MODE=1
VLLM_SLEEP_WHEN_IDLE=1"></textarea>
                    <div class="help-text">Format: KEY=VALUE (one per line)</div>
                </div>

                <div class="section-title backend-specific vllm-settings active">üõ†Ô∏è Custom vLLM Arguments</div>
                <div class="form-group backend-specific vllm-settings active">
                    <label>Custom vLLM Arguments</label>
                    <textarea id="customVllmArgs" rows="4" placeholder="--served-model-name MyModel"></textarea>
                </div>

                <div class="section-title backend-specific llamacpp-settings">üõ†Ô∏è Custom llama.cpp Arguments</div>
                <div class="form-group backend-specific llamacpp-settings">
                    <label>Custom llama.cpp Arguments</label>
                    <textarea id="customLlamacppArgs" rows="4" placeholder="--mlock
--numa"></textarea>
                </div>

                <div class="section-title">üê≥ Custom Docker Arguments</div>
                <div class="form-group">
                    <label>Custom Docker Arguments</label>
                    <textarea id="customDockerArgs" rows="3" placeholder="ulimit_memlock=-1"></textarea>
                </div>

                <div class="form-actions">
                    <button type="submit" class="btn-submit">Save</button>
                    <button type="button" class="btn-cancel" onclick="closeModal()">Cancel</button>
                </div>
            </form>
        </div>
    </div>

    <script>
        let editingModel = null;

        function toggleBackendSettings() {
            const backend = document.getElementById('backend').value;
            const vllmSettings = document.querySelectorAll('.vllm-settings');
            const llamacppSettings = document.querySelectorAll('.llamacpp-settings');

            if (backend === 'vllm') {
                vllmSettings.forEach(el => el.classList.add('active'));
                llamacppSettings.forEach(el => el.classList.remove('active'));
            } else {
                vllmSettings.forEach(el => el.classList.remove('active'));
                llamacppSettings.forEach(el => el.classList.add('active'));
            }
        }

        async function loadModels() {
            try {
                const response = await fetch('/api/models');
                const models = await response.json();
                const grid = document.getElementById('modelsGrid');
                grid.innerHTML = '';
                models.forEach(model => grid.appendChild(createModelCard(model)));
            } catch (error) {
                console.error('Error:', error);
            }
        }

        function createModelCard(model) {
            const card = document.createElement('div');
            card.className = 'model-card';

            const statusClass = model.status === 'running' ? 'status-running' : 
                              model.status === 'paused' ? 'status-paused' : 'status-stopped';
            const statusText = model.status === 'running' ? 'Running' :
                             model.status === 'paused' ? 'Paused' : 'Stopped';

            const stats = model.stats || {};
            const tps = stats.average_tokens_per_second || 0;
            const tpsDisplay = tps > 0 ? tps.toFixed(2) + ' t/s' : 'N/A';

            let badges = '';
            const backend = model.config.backend || 'vllm';
            badges += '<span class="info-badge badge-' + backend + '">' + backend.toUpperCase() + '</span>';
            if (model.config.preload) badges += '<span class="info-badge badge-preload">PRELOAD</span>';
            if (model.config.exclusive) badges += '<span class="info-badge badge-exclusive">EXCLUSIVE</span>';
            if (model.config.auto_sleep) badges += '<span class="info-badge badge-autosleep">AUTO-SLEEP</span>';
            if (model.config.always_visible) badges += '<span class="info-badge badge-visible">VISIBLE</span>';
            if (model.config.docker_image) badges += '<span class="info-badge badge-custom">CUSTOM IMG</span>';
            if (model.config.custom_env_vars) badges += '<span class="info-badge badge-env">CUSTOM ENV</span>';

            const dockerImg = model.config.docker_image || (backend === 'llamacpp' ? 'ghcr.io/ggerganov/llama.cpp:server' : 'vllm/vllm-openai:v0.13.0');
            const dockerImgShort = dockerImg.split('/').pop();

            const startBtn = model.status !== 'running' ? 
                '<button class="btn btn-start" onclick="startModel(\'' + model.name + '\')">‚ñ∂ Start</button>' :
                '<button class="btn btn-stop" onclick="stopModel(\'' + model.name + '\')">‚èπ Stop</button>';
            const sleepBtn = model.status === 'running' ?
                '<button class="btn btn-sleep" onclick="sleepModel(\'' + model.name + '\')">üí§ Sleep</button>' : '';
            const wakeBtn = model.status === 'paused' ?
                '<button class="btn btn-wake" onclick="wakeModel(\'' + model.name + '\')">‚ö° Wake</button>' : '';

            let envInfo = '';
            if (model.config.custom_env_vars) {
                envInfo = '<div class="model-info-item"><span class="label">üåç ENV:</span><span class="value" style="color: #6366f1; font-weight: bold;">‚úì Custom</span></div>';
            }

            card.innerHTML = '<div class="model-header">' +
                '<div class="model-name">' + model.name + badges + '</div>' +
                '<div class="status-badge ' + statusClass + '">' + statusText + '</div>' +
                '</div>' +
                '<div class="model-info">' +
                '<div class="model-info-item"><span class="label">üìÇ Path:</span><span class="value">' + model.config.model_path + '</span></div>' +
                '<div class="model-info-item"><span class="label">üîå Port:</span><span class="value">' + model.config.port + '</span></div>' +
                '<div class="model-info-item"><span class="label">üê≥ Image:</span><span class="value">' + dockerImgShort + '</span></div>' +
                envInfo +
                '</div>' +
                '<div class="stats-section">' +
                '<div class="stats-title">üìä Statistics</div>' +
                '<div class="stat-item"><span class="stat-label">Requests:</span><span class="stat-value">' + (stats.total_requests || 0) + '</span></div>' +
                '<div class="stat-item"><span class="stat-label">Speed:</span><span class="stat-value stat-highlight">' + tpsDisplay + '</span></div>' +
                '<div class="stat-item"><span class="stat-label">Tokens:</span><span class="stat-value">' + ((stats.total_tokens || 0).toLocaleString()) + '</span></div>' +
                '</div>' +
                '<div class="model-actions">' +
                startBtn + sleepBtn + wakeBtn +
                '<button class="btn btn-edit" onclick="editModel(\'' + model.name + '\')">‚úèÔ∏è</button>' +
                '<button class="btn btn-delete" onclick="deleteModel(\'' + model.name + '\')">üóëÔ∏è</button>' +
                '</div>';

            return card;
        }

        async function checkHealth() {
            const r = await fetch('/health');
            const data = await r.json();
            alert('Health: ' + data.status);
        }

        async function showRunning() {
            const r = await fetch('/running');
            const data = await r.json();
            const msg = 'Running Models: ' + data.count + '\n\n' + 
                       data.models.map(m => m.name + ' (port ' + m.port + ') - uptime: ' + Math.floor(m.uptime) + 's').join('\n');
            alert(msg || 'No models running');
        }

        async function startModel(name) {
            const r = await fetch('/api/models/' + name + '/start', { method: 'POST' });
            alert((await r.json()).message);
            loadModels();
        }

        async function stopModel(name) {
            if (!confirm('Stop ' + name + '?')) return;
            const r = await fetch('/api/models/' + name + '/stop', { method: 'POST' });
            alert((await r.json()).message);
            loadModels();
        }

        async function sleepModel(name) {
            const r = await fetch('/api/models/' + name + '/sleep', { method: 'POST' });
            alert((await r.json()).message);
            loadModels();
        }

        async function wakeModel(name) {
            const r = await fetch('/api/models/' + name + '/wake', { method: 'POST' });
            alert((await r.json()).message);
            loadModels();
        }

        async function deleteModel(name) {
            if (!confirm('Delete ' + name + '?')) return;
            const r = await fetch('/api/models/' + name, { method: 'DELETE' });
            alert((await r.json()).message);
            loadModels();
        }

        function openAddModal() {
            editingModel = null;
            document.getElementById('modalTitle').textContent = 'Add New Model';
            document.getElementById('modelForm').reset();
            toggleBackendSettings();
            document.getElementById('modelModal').classList.add('active');
        }

        async function editModel(name) {
            const r = await fetch('/api/models');
            const models = await r.json();
            const model = models.find(m => m.name === name);
            if (!model) return;

            editingModel = name;
            document.getElementById('modalTitle').textContent = 'Edit: ' + name;
            document.getElementById('modelName').value = model.name;
            document.getElementById('backend').value = model.config.backend || 'vllm';
            toggleBackendSettings();
            document.getElementById('dockerImage').value = model.config.docker_image || '';
            document.getElementById('modelPath').value = model.config.model_path;
            document.getElementById('isLocal').checked = model.config.is_local || false;
            document.getElementById('port').value = model.config.port;
            document.getElementById('gpuMemory').value = model.config.gpu_memory_utilization || 0.9;
            document.getElementById('maxModelLen').value = model.config.max_model_len || '';
            document.getElementById('quantization').value = model.config.quantization || '';
            document.getElementById('tensorParallel').value = model.config.tensor_parallel_size || 1;
            document.getElementById('contextSize').value = model.config.context_size || 4096;
            document.getElementById('gpuLayers').value = model.config.gpu_layers !== undefined ? model.config.gpu_layers : 35;
            document.getElementById('parallelRequests').value = model.config.parallel_requests || 4;
            document.getElementById('preload').checked = model.config.preload || false;
            document.getElementById('exclusive').checked = model.config.exclusive || false;
            document.getElementById('autoSleep').checked = model.config.auto_sleep !== false;
            document.getElementById('sleepTimeout').value = model.config.sleep_timeout || 300;
            document.getElementById('alwaysVisible').checked = model.config.always_visible || false;
            document.getElementById('customEnvVars').value = model.config.custom_env_vars || '';
            document.getElementById('customVllmArgs').value = model.config.custom_vllm_args || '';
            document.getElementById('customLlamacppArgs').value = model.config.custom_llamacpp_args || '';
            document.getElementById('customDockerArgs').value = model.config.custom_docker_args || '';
            document.getElementById('modelModal').classList.add('active');
        }

        function closeModal() {
            document.getElementById('modelModal').classList.remove('active');
        }

        async function saveModel(e) {
            e.preventDefault();
            const backend = document.getElementById('backend').value;
            const config = {
                name: document.getElementById('modelName').value,
                backend: backend,
                docker_image: document.getElementById('dockerImage').value || null,
                model_path: document.getElementById('modelPath').value,
                is_local: document.getElementById('isLocal').checked,
                port: parseInt(document.getElementById('port').value),
                preload: document.getElementById('preload').checked,
                exclusive: document.getElementById('exclusive').checked,
                auto_sleep: document.getElementById('autoSleep').checked,
                sleep_timeout: parseInt(document.getElementById('sleepTimeout').value),
                always_visible: document.getElementById('alwaysVisible').checked,
                custom_env_vars: document.getElementById('customEnvVars').value || null,
                custom_docker_args: document.getElementById('customDockerArgs').value || null
            };

            if (backend === 'vllm') {
                config.gpu_memory_utilization = parseFloat(document.getElementById('gpuMemory').value);
                config.max_model_len = document.getElementById('maxModelLen').value ? parseInt(document.getElementById('maxModelLen').value) : null;
                config.quantization = document.getElementById('quantization').value || null;
                config.tensor_parallel_size = parseInt(document.getElementById('tensorParallel').value);
                config.custom_vllm_args = document.getElementById('customVllmArgs').value || null;
                config.custom_llamacpp_args = null;
                config.context_size = null;
                config.gpu_layers = null;
                config.parallel_requests = null;
            } else {
                config.context_size = parseInt(document.getElementById('contextSize').value) || 4096;
                config.gpu_layers = parseInt(document.getElementById('gpuLayers').value);
                config.parallel_requests = parseInt(document.getElementById('parallelRequests').value) || 4;
                config.custom_llamacpp_args = document.getElementById('customLlamacppArgs').value || null;
                config.custom_vllm_args = null;
                config.gpu_memory_utilization = null;
                config.max_model_len = null;
                config.quantization = null;
                config.tensor_parallel_size = null;
            }

            const url = editingModel ? '/api/models/' + editingModel : '/api/models';
            const method = editingModel ? 'PUT' : 'POST';
            const r = await fetch(url, {
                method: method,
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(config)
            });
            const result = await r.json();
            alert(result.message || result.detail);
            if (r.ok) {
                closeModal();
                loadModels();
            }
        }

        async function exportConfig() {
            const r = await fetch('/api/config/export', { method: 'POST' });
            const config = await r.json();
            const blob = new Blob([JSON.stringify(config, null, 2)], { type: 'application/json' });
            const url = URL.createObjectURL(blob);
            const a = document.createElement('a');
            a.href = url;
            a.download = 'llm-proxy-config.json';
            a.click();
            URL.revokeObjectURL(url);
        }

        async function importConfig(event) {
            const file = event.target.files[0];
            if (!file) return;
            const text = await file.text();
            const config = JSON.parse(text);
            if (!confirm('Replace config?')) return;
            const r = await fetch('/api/config/import', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(config)
            });
            alert((await r.json()).message);
            loadModels();
            event.target.value = '';
        }

        loadModels();
        setInterval(loadModels, 10000);
    </script>
</body>
</html>
